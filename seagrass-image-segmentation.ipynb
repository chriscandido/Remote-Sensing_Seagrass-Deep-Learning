{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU wandb\n!pip install -q segmentation-models-pytorch \n!pip install -q torchsummary\n!pip install -q rasterio\n!pip install -q colorama","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-05T08:17:31.452903Z","iopub.execute_input":"2022-09-05T08:17:31.453620Z","iopub.status.idle":"2022-09-05T08:18:34.638352Z","shell.execute_reply.started":"2022-09-05T08:17:31.453177Z","shell.execute_reply":"2022-09-05T08:18:34.637107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:34.641892Z","iopub.execute_input":"2022-09-05T08:18:34.642266Z","iopub.status.idle":"2022-09-05T08:18:34.686593Z","shell.execute_reply.started":"2022-09-05T08:18:34.642230Z","shell.execute_reply":"2022-09-05T08:18:34.685712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IMPORT LIBRARIES","metadata":{}},{"cell_type":"code","source":"# Misc\nimport copy\nimport time\nimport timm\nimport joblib\nimport random\nimport os, shutil\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom IPython import display as ipd\nfrom collections import defaultdict\nfrom joblib import Parallel, delayed\nfrom matplotlib.patches import Rectangle \n\n# Sklearn\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# Pytorch\nimport torch \nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom torch.cuda import amp\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms as T\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchsummary import summary\nimport segmentation_models_pytorch as smp \n\n# Image Processing Libraries\nimport cv2\nimport rasterio \nimport skimage.io\nimport io\n\nfrom PIL import Image\n\n# Albumentations for augmentation\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:34.687885Z","iopub.execute_input":"2022-09-05T08:18:34.688221Z","iopub.status.idle":"2022-09-05T08:18:40.846156Z","shell.execute_reply.started":"2022-09-05T08:18:34.688183Z","shell.execute_reply":"2022-09-05T08:18:40.845044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import and Login to Weight&Biases \nimport wandb\n\ntry:\n    wandb.login(key='29b54518076f707810c9bac855d80a73e7e057a5') # API Key\n    anonymous = None\nexcept:\n    anonymous = \"must\"\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:40.848530Z","iopub.execute_input":"2022-09-05T08:18:40.849219Z","iopub.status.idle":"2022-09-05T08:18:45.295384Z","shell.execute_reply.started":"2022-09-05T08:18:40.849178Z","shell.execute_reply":"2022-09-05T08:18:45.294282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:45.297047Z","iopub.execute_input":"2022-09-05T08:18:45.297521Z","iopub.status.idle":"2022-09-05T08:18:45.545191Z","shell.execute_reply.started":"2022-09-05T08:18:45.297437Z","shell.execute_reply":"2022-09-05T08:18:45.544062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CONFIGURATIONS","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed          = 101\n    debug         = False # set debug=False for Full Training\n    exp_name      = 'Baselinev1'\n    comment       = 'Linknet-Resnet152-512x512-aug2-split2'\n    model_name    = 'Linknet'\n    encoder       = 'resnet152'\n    train_bs      = 5\n    valid_bs      = train_bs\n    img_size      = [512, 512]\n    size          = 256\n    epochs        = 20\n    lr            = 1e-3\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(10000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-4\n    n_fold        = 5\n    num_classes   = 2\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:52.746167Z","iopub.execute_input":"2022-09-05T08:18:52.747018Z","iopub.status.idle":"2022-09-05T08:18:52.799987Z","shell.execute_reply.started":"2022-09-05T08:18:52.746979Z","shell.execute_reply":"2022-09-05T08:18:52.798853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## REPRODUCIBILITY","metadata":{}},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:53.918257Z","iopub.execute_input":"2022-09-05T08:18:53.918629Z","iopub.status.idle":"2022-09-05T08:18:53.976065Z","shell.execute_reply.started":"2022-09-05T08:18:53.918594Z","shell.execute_reply":"2022-09-05T08:18:53.974243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IMAGE PRE-PROCESSING","metadata":{}},{"cell_type":"markdown","source":"#### Load Data","metadata":{}},{"cell_type":"code","source":"# Image and Mask Path\nIMAGE_PATH = '../input/seagrass/train_images/training_images/'\nMASK_PATH = '../input/seagrass/train_label/training_label/'","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:56.148644Z","iopub.execute_input":"2022-09-05T08:18:56.149291Z","iopub.status.idle":"2022-09-05T08:18:56.198931Z","shell.execute_reply.started":"2022-09-05T08:18:56.149253Z","shell.execute_reply":"2022-09-05T08:18:56.197877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = 2\n\ndef create_df():\n    img_name = []\n    for dirname, _, filenames in os.walk(IMAGE_PATH):\n        for filename in filenames:\n            img_name.append(filename.split('.')[0])\n\n    mask_name = []\n    for dirname, _, filenames in os.walk(MASK_PATH):\n        for filename in filenames:\n            mask_name.append(filename.split('.')[0])\n            \n    img_name = sorted(img_name)\n    mask_name = sorted(mask_name)\n\n    return pd.DataFrame({'img_id': img_name, 'mask_id': mask_name}, index = np.arange(0, len(img_name)))\n\ndf = create_df()\nprint('Total Images: ', len(df))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:56.638838Z","iopub.execute_input":"2022-09-05T08:18:56.639444Z","iopub.status.idle":"2022-09-05T08:18:56.996713Z","shell.execute_reply.started":"2022-09-05T08:18:56.639407Z","shell.execute_reply":"2022-09-05T08:18:56.995661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df # list of drone images","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:18:57.082498Z","iopub.execute_input":"2022-09-05T08:18:57.083538Z","iopub.status.idle":"2022-09-05T08:18:57.146855Z","shell.execute_reply.started":"2022-09-05T08:18:57.083491Z","shell.execute_reply":"2022-09-05T08:18:57.145811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Split data to Train, Test, and Validation","metadata":{}},{"cell_type":"code","source":"# Split data\nX_trainval, X_test = train_test_split(df['img_id'].values, test_size=0.1, random_state=19)\nX_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n\nprint('Train Size   : ', len(X_train))\nprint('Val Size     : ', len(X_val))\nprint('Test Size    : ', len(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:00.053904Z","iopub.execute_input":"2022-09-05T08:19:00.054776Z","iopub.status.idle":"2022-09-05T08:19:00.110576Z","shell.execute_reply.started":"2022-09-05T08:19:00.054713Z","shell.execute_reply":"2022-09-05T08:19:00.109565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = skimage.io.imread(IMAGE_PATH + df['img_id'][500] + '.tif', plugin='tifffile')\nmask = skimage.io.imread(MASK_PATH + df['mask_id'][500] + '.tif', plugin='tifffile')\nprint('Image Size', np.asarray(img).shape)\nprint('Mask Size', np.asarray(mask).shape)\n\n\nplt.imshow(img)\nplt.imshow(mask, alpha=0.30)\nplt.title('Picture with Mask Appplied')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:00.461921Z","iopub.execute_input":"2022-09-05T08:19:00.462913Z","iopub.status.idle":"2022-09-05T08:19:00.862368Z","shell.execute_reply.started":"2022-09-05T08:19:00.462862Z","shell.execute_reply":"2022-09-05T08:19:00.861382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DroneDataset(Dataset):\n    \n    def __init__(self, img_path, mask_path, X, transform=None, patch=False):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.mean = [0.4855, 0.5464, 0.4754]\n        self.std = [0.1446, 0.1593, 0.1308]\n        self.transform = transform\n        self.patches = patch\n        self.count = 0\n   \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        \n        img = skimage.io.imread(self.img_path + self.X[idx] + '.tif', plugin='tifffile')\n        img = np.array(img, dtype='uint8')\n        img = cv2.resize(img, dsize=(CFG.size,CFG.size), interpolation=cv2.INTER_AREA)\n        \n        mask = skimage.io.imread(self.mask_path + self.X[idx] + '.tif', plugin='tifffile')\n        mask = np.array(mask, dtype='uint8')\n        mask = cv2.resize(mask, dsize=(CFG.size,CFG.size), interpolation=cv2.INTER_AREA)\n\n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = Image.fromarray(aug['image'])\n            mask = aug['mask']\n        \n        if self.transform is None:\n            img = Image.fromarray(img)\n        \n        t = T.Compose([T.ToTensor()])\n        mean, std = t(img).mean([1,2]), t(img).std([1,2])\n\n        transform = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n        img = transform(img)\n        \n        #t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n        #img = t(img)\n      \n        mask = torch.from_numpy(mask).long()\n        \n        if self.patches:\n            img, mask = self.tiles(img, mask)\n            \n        return img, mask\n    \n    def tiles(self, img, mask):\n\n        img_patches = img.unfold(1, CFG.size, CFG.size).unfold(2, CFG.size, CFG.size) \n        img_patches  = img_patches.contiguous().view(3,-1, CFG.size, CFG.size) \n        img_patches = img_patches.permute(1,0,2,3)\n        \n        mask_patches = mask.unfold(0, CFG.size, CFG.size).unfold(1, CFG.size, CFG.size)\n        mask_patches = mask_patches.contiguous().view(-1, CFG.size, CFG.size)\n        \n        return img_patches, mask_patches","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:02.598263Z","iopub.execute_input":"2022-09-05T08:19:02.598671Z","iopub.status.idle":"2022-09-05T08:19:02.658202Z","shell.execute_reply.started":"2022-09-05T08:19:02.598635Z","shell.execute_reply":"2022-09-05T08:19:02.657092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data augmentation","metadata":{}},{"cell_type":"code","source":"# Data Augmentation\nt_train = A.Compose([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), \n                     A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n                     A.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25, p=0.75),\n                     A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, p=0.5), \n                     A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n                     A.GaussNoise()])\n\nt_val = A.Compose([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), \n                   A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, p=0.5)])\n\n#datasets\n\ntrain_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train, t_train, patch=False)\ntrain_loader = DataLoader(train_set, batch_size=CFG.train_bs, shuffle=True, num_workers=0, pin_memory=True)\n\nval_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, t_val, patch=False)\nval_loader = DataLoader(val_set, batch_size=CFG.valid_bs, shuffle=True, num_workers=0, pin_memory=True)  ","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:04.772345Z","iopub.execute_input":"2022-09-05T08:19:04.772716Z","iopub.status.idle":"2022-09-05T08:19:04.828728Z","shell.execute_reply.started":"2022-09-05T08:19:04.772683Z","shell.execute_reply":"2022-09-05T08:19:04.827539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs, msks = next(iter(train_loader))\nimgs.size(), msks.size()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:06.409361Z","iopub.execute_input":"2022-09-05T08:19:06.409753Z","iopub.status.idle":"2022-09-05T08:19:09.872973Z","shell.execute_reply.started":"2022-09-05T08:19:06.409703Z","shell.execute_reply":"2022-09-05T08:19:09.871813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODEL","metadata":{}},{"cell_type":"markdown","source":"#### 1. Unet \n#### 2. Unet++\n#### 3. Linknet\n#### 4. FPN\n#### 5. DeepLabV3","metadata":{}},{"cell_type":"code","source":"def build_model():\n    \n    if CFG.model_name == 'Unet':\n        \n        model = smp.Unet(\n            encoder_name = CFG.encoder,\n            encoder_weights = 'imagenet',\n            encoder_depth = 5,\n            classes = CFG.num_classes,\n            activation = 'sigmoid',\n            decoder_channels = [256, 128, 64, 32, 16]\n        )\n            \n    elif CFG.model_name == 'UnetPlusPlus':\n        \n        model = smp.UnetPlusPlus(\n            encoder_name = CFG.encoder,\n            encoder_weughts = 'imagenet',\n            encoder_depth = 5,\n            classes = CFG.num_classes,\n            activation = 'sigmoid',\n            decoder_channels = [256, 128, 64, 32, 16]\n        )\n    \n    elif CFG.model_name == 'Linknet':\n        \n        model = smp.Linknet(\n            encoder_name = CFG.encoder,\n            encoder_weights = 'imagenet',\n            encoder_depth = 5,\n            classes = CFG.num_classes,\n            activation = 'sigmoid',\n            decoder_use_batchnorm = True\n        )\n    \n    elif CFG.model_name == 'FPN':\n        \n        model = smp.FPN(\n            encoder_name = CFG.encoder,\n            encoder_weights = 'imagenet',\n            encoder_depth = 5,\n            classes = CFG.num_classes,\n            activation = 'sigmoid',\n            decoder_pyramid_channels = 256\n        )\n    \n    elif CFG.model_name == 'DeepLabV3':\n        \n        model = smp.DeepLabV3(\n            encoder_name = CFG.encoder,\n            encoder_weights = 'imagenet',\n            encoder_depth = 5,\n            classes = CFG.num_classes,\n            activation = 'sigmoid',\n            decoder_channels = 256\n        )\n        \n    model.to(CFG.device)\n    return model\n\ndef load_model(path):\n\n    model = build_model()\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loss Function","metadata":{}},{"cell_type":"code","source":"# Loss Function\nCrossEntropyLoss = nn.CrossEntropyLoss()\nBCELoss = nn.BCELoss() \n\n# Metrics\ndef pixel_accuracy(output, mask):\n    with torch.no_grad():\n        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n        correct = torch.eq(output, mask).int()\n        accuracy = float(correct.sum()) / float(correct.numel())\n    return accuracy\n\ndef dice_coef(pred_mask, mask, smooth = 1):\n    with torch.no_grad():\n        pred_mask = F.softmax(pred_mask, dim=1)\n        pred_mask = torch.argmax(pred_mask, dim=1)\n        pred_mask = pred_mask.contiguous().view(-1)\n        mask = mask.contiguous().view(-1)\n        \n        mask = torch.flatten(mask).to(torch.float32)\n        pred_mask = torch.flatten(pred_mask).to(torch.float32)\n        \n        intersection = torch.sum(mask * pred_mask)\n        score = (2. * intersection + smooth) / (torch.sum(mask) + torch.sum(pred_mask) + smooth) \n        \n    return score.cpu().numpy()\n\ndef mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n    with torch.no_grad():\n        pred_mask = F.softmax(pred_mask, dim=1)\n        pred_mask = torch.argmax(pred_mask, dim=1)\n        pred_mask = pred_mask.contiguous().view(-1)\n        mask = mask.contiguous().view(-1)\n\n        iou_per_class = []\n        for clas in range(0, n_classes): #loop per pixel class\n            true_class = pred_mask == clas\n            true_label = mask == clas\n\n            if true_label.long().sum().item() == 0: #no exist label in this loop\n                iou_per_class.append(np.nan)\n            else:\n                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n                union = torch.logical_or(true_class, true_label).sum().float().item()\n\n                iou = (intersect + smooth) / (union +smooth)\n                iou_per_class.append(iou)\n                \n    return np.nanmean(iou_per_class)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:18.576156Z","iopub.execute_input":"2022-09-05T08:19:18.577471Z","iopub.status.idle":"2022-09-05T08:19:18.658969Z","shell.execute_reply.started":"2022-09-05T08:19:18.577427Z","shell.execute_reply":"2022-09-05T08:19:18.657818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train One Epoch Function","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, criterion, dataloader, device, epoch, patch=False):\n\n    model.train()\n\n    scaler = amp.GradScaler()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    train_scores = []\n\n    pbar = tqdm(enumerate(dataloader), total = len(dataloader), desc = 'Train')\n    for steps, (images, masks) in pbar:\n\n        images = images.to(device)\n        masks = masks.to(device)\n\n        batch_size = images.size(0)\n\n        y_pred = model(images)\n        loss = criterion(y_pred, masks)\n        \n        loss.backward()\n        \n        # Zero the parameter gradients\n        optimizer.step()  \n        optimizer.zero_grad()\n        \n        scheduler.step()\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n        \n        train_iou = mIoU(y_pred, masks)\n        train_acc = pixel_accuracy(y_pred, masks)\n        train_dce = dice_coef(y_pred, masks)\n        train_scores.append([train_iou, train_acc, train_dce]) \n\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss = f'{epoch_loss:0.4f}',\n                        lr = f'{current_lr:0.5f}',\n                        gpu_mem = f'{mem:0.2f} GB')\n    \n    train_scores = np.mean(train_scores, axis = 0)\n      \n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return epoch_loss, train_scores","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:19.141933Z","iopub.execute_input":"2022-09-05T08:19:19.143268Z","iopub.status.idle":"2022-09-05T08:19:19.221775Z","shell.execute_reply.started":"2022-09-05T08:19:19.143225Z","shell.execute_reply":"2022-09-05T08:19:19.220783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validate One Epoch Function","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, criterion, device, epoch):\n\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    val_scores = []\n\n    pbar = tqdm(enumerate(dataloader), total = len(dataloader), desc = 'Validation')\n    for steps, (images, masks) in pbar:\n\n        images = images.to(device)\n        masks = masks.to(device)\n\n        batch_size = images.size(0)\n\n        y_pred = model(images)\n        loss = criterion(y_pred, masks)\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        val_iou = mIoU(y_pred, masks)\n        val_acc = pixel_accuracy(y_pred, masks)\n        val_dce = dice_coef(y_pred, masks)\n        val_scores.append([val_iou, val_acc, val_dce])\n\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss = f'{epoch_loss:0.4f}',\n                          lr = f'{current_lr:0.5f}',\n                          gpu_memory = f'{mem:0.2f} GB')\n      \n    val_scores = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return epoch_loss, val_scores","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:19.869287Z","iopub.execute_input":"2022-09-05T08:19:19.869907Z","iopub.status.idle":"2022-09-05T08:19:19.950308Z","shell.execute_reply.started":"2022-09-05T08:19:19.869851Z","shell.execute_reply":"2022-09-05T08:19:19.948816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run Training Function","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, criterion, device, epochs):\n    \n    # log gradients to wandb\n    wandb.watch(model, criterion, log='all', log_freq=100)\n\n    if torch.cuda.is_available():\n        print (\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_iou = -np.inf\n    best_dce = -np.inf\n    best_acc = -np.inf\n    best_epoch = -1\n    history = defaultdict(list)\n\n    for epoch in range(1, epochs + 1):\n        gc.collect()\n        print(f'Epoch {epoch}/{epochs}', end = '')\n        \n        train_loss, train_scores = train_one_epoch(model, optimizer, scheduler, criterion, \n                                     dataloader = train_loader,\n                                     device = CFG.device, epoch = epoch)\n\n        val_loss, val_scores = valid_one_epoch(model, val_loader, criterion, \n                                               device = CFG.device, epoch = epoch)\n        train_iou, train_acc, train_dce = train_scores\n        val_iou, val_acc, val_dce = val_scores\n\n        history['Train Loss'].append(train_loss)\n        history['Train IoU'].append(train_iou)\n        history['Train Pixel Accuracy'].append(train_acc)\n        history['Train Dice'].append(train_dce)\n        history['Valid Loss'].append(val_loss)\n        history['Valid IoU'].append(val_iou)\n        history['Valid Pixel Accuracy'].append(val_acc)\n        history['Valid Dice'].append(val_dce)\n      \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_loss,\n                   \"Train Iou\": train_iou,\n                   \"Train Pixel Accuracy\": train_acc,\n                   \"Train Dice\": train_dce,\n                   \"Valid Loss\": val_loss,\n                   \"Valid IoU\": val_iou,\n                   \"Valid Pixel Accuracy\": val_acc,\n                   \"Valid Dice\": val_dce,\n                   \"LR\":scheduler.get_last_lr()[0]})\n\n        print(f'Train IoU: {train_iou:0.4f} | Train Pixel Accuracy: {train_acc:0.4f} | Train Dice: {train_dce:0.4f}')\n        print(f'Valid IoU: {val_iou:0.4f} | Valid Pixel Accuracy: {val_acc:0.4f} | Valid Dice: {val_dce:0.4f}')\n  \n\n        # Deep copy model \n        if val_iou >= best_iou:\n            print(f\"{c_}Valid IoU Improved ({best_iou:0.4f} ---> {val_iou:0.4f})\")\n            print(f\"{c_}Valid Dice Coef Improved ({best_dce:0.4f} ---> {val_dce:0.4f})\")\n            print(f\"{c_}Valid Pixel Accuracy Improved ({best_acc:0.4f} ---> {val_acc:0.4f})\")\n            best_iou = val_iou\n            best_dce = val_dce\n            best_acc = val_acc\n            best_epoch = epoch\n            run.summary[\"Best IoU\"] = best_iou\n            run.summary[\"Best Dice Coef\"] = best_dce\n            run.summary[\"Best Accuracy\"] = best_acc\n            run.summary[\"Best Epoch\"] = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch-{best_epoch:02d}.bin\"\n            torch.save(model.state_dict(), PATH)\n            wandb.save(PATH)\n            print(f\"Model Saved{sr_}\")\n        \n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch-{epoch:2d}.bin\"\n        torch.save(model.state_dict(), PATH)\n\n        print() ; print()\n\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print('Best Score: {:.4f}'.format(best_iou))\n\n    model.load_state_dict(best_model_wts)\n\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:20.614727Z","iopub.execute_input":"2022-09-05T08:19:20.615164Z","iopub.status.idle":"2022-09-05T08:19:20.726824Z","shell.execute_reply.started":"2022-09-05T08:19:20.615126Z","shell.execute_reply":"2022-09-05T08:19:20.723881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scheduler Function","metadata":{}},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CFG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max, \n                                                   eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0, \n                                                             eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=CFG.min_lr,)\n    elif CFG.scheduer == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif CFG.scheduler == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:21.373123Z","iopub.execute_input":"2022-09-05T08:19:21.373949Z","iopub.status.idle":"2022-09-05T08:19:21.450027Z","shell.execute_reply.started":"2022-09-05T08:19:21.373904Z","shell.execute_reply":"2022-09-05T08:19:21.448912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()\noptimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\nscheduler = fetch_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:22.183634Z","iopub.execute_input":"2022-09-05T08:19:22.184343Z","iopub.status.idle":"2022-09-05T08:19:25.426774Z","shell.execute_reply.started":"2022-09-05T08:19:22.184307Z","shell.execute_reply":"2022-09-05T08:19:25.425776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING","metadata":{}},{"cell_type":"code","source":"run = wandb.init(project='bluecares-seagrass-dl', \n                config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n                anonymous=anonymous,\n                name=f\"model-{CFG.model_name}|encoder-{CFG.encoder}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}\",\n                group=CFG.comment,\n)\n\nmodel = build_model()\noptimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\nscheduler = fetch_scheduler(optimizer)\n\n#optimizer = torch.optim.AdamW(model.parameters(), lr = CFG.lr, weight_decay=CFG.wd)\n#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, CFG.lr, epochs=CFG.epochs,\n#                                            steps_per_epoch=len(train_loader))\n\nmodel, history = run_training(model, optimizer, scheduler, CrossEntropyLoss, \n                  device=CFG.device,\n                  epochs=CFG.epochs)\n# run.finish()\ndisplay(ipd.IFrame(run.url, width=1000, height=720)) ","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:19:29.053189Z","iopub.execute_input":"2022-09-05T08:19:29.053577Z","iopub.status.idle":"2022-09-05T08:49:30.308985Z","shell.execute_reply.started":"2022-09-05T08:19:29.053540Z","shell.execute_reply":"2022-09-05T08:49:30.308081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOAD VALIDATION DATASET","metadata":{}},{"cell_type":"code","source":"class DroneTestDataset(Dataset):\n    \n    def __init__(self, img_path, mask_path, X, transform=None):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n      \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n\n        img = skimage.io.imread(self.img_path + self.X[idx] + '.tif', plugin='tifffile')\n        img = np.array(img, dtype='uint8')\n        img = cv2.resize(img, dsize=(CFG.size,CFG.size), interpolation=cv2.INTER_AREA)\n        \n        mask = skimage.io.imread(self.mask_path + self.X[idx] + '.tif', plugin='tifffile')\n        mask = np.array(mask, dtype='uint8')\n        mask = cv2.resize(mask, dsize=(CFG.size,CFG.size), interpolation=cv2.INTER_AREA)\n        \n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = Image.fromarray(aug['image'])\n            mask = aug['mask']\n        \n        if self.transform is None:\n            img = Image.fromarray(img)\n        \n        mask = torch.from_numpy(mask).long()\n        \n        return img, mask\n\n\nt_test = A.Resize(CFG.size, CFG.size, interpolation=cv2.INTER_AREA)\ntest_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-05T09:01:13.289642Z","iopub.execute_input":"2022-09-05T09:01:13.291295Z","iopub.status.idle":"2022-09-05T09:01:13.617203Z","shell.execute_reply.started":"2022-09-05T09:01:13.291244Z","shell.execute_reply":"2022-09-05T09:01:13.615912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREDICTION ","metadata":{}},{"cell_type":"markdown","source":"#### IoU","metadata":{}},{"cell_type":"code","source":"# IoU\ndef predict_image_mask_miou(model, image, mask, mean=[0.4855, 0.5464, 0.4754], std=[0.1446, 0.1593, 0.1308]):\n    model.eval()\n    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n    image = t(image)\n    model.to(device); image=image.to(device)\n    mask = mask.to(device)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        score = mIoU(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0).numpy()\n        \n    return masked, score","metadata":{"execution":{"iopub.status.busy":"2022-09-05T09:01:20.020178Z","iopub.execute_input":"2022-09-05T09:01:20.021170Z","iopub.status.idle":"2022-09-05T09:01:20.076280Z","shell.execute_reply.started":"2022-09-05T09:01:20.021130Z","shell.execute_reply":"2022-09-05T09:01:20.075212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pixel Accuracy","metadata":{}},{"cell_type":"code","source":"# Pixel Accuracy\ndef predict_image_mask_pixel(model, image, mask, mean=[0.4855, 0.5464, 0.4754], std=[0.1446, 0.1593, 0.1308]):\n    model.eval()\n    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n    image = t(image)\n    model.to(device); image=image.to(device)\n    mask = mask.to(device)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        acc = pixel_accuracy(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0).numpy()\n        \n    return masked, acc","metadata":{"execution":{"iopub.status.busy":"2022-09-05T09:01:20.793202Z","iopub.execute_input":"2022-09-05T09:01:20.794355Z","iopub.status.idle":"2022-09-05T09:01:20.849488Z","shell.execute_reply.started":"2022-09-05T09:01:20.794305Z","shell.execute_reply":"2022-09-05T09:01:20.848480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL\ndata = []\nscores = []\n\ntable = wandb.Table(columns=['IMAGE', 'GROUND_TRUTH','PREDICTED_MASK', 'IoU', 'PIXEL_ACCURACY'])\n\npbar = tqdm(enumerate(test_set), total = len(test_set), desc = 'Prediction')\nfor steps, (image2, mask2) in pbar:\n    \n    pred_mask2, score2 = predict_image_mask_miou(model, image2, mask2)\n    pred_mask3, score3 = predict_image_mask_pixel(model, image2, mask2)\n    \n    mask2 = mask2.cpu().squeeze(0).numpy()\n        \n    #fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\n    #ax1.imshow(image2)\n    #ax1.set_title('Picture');\n    \n    #ax2.imshow(mask2)\n    #ax2.set_title('Ground truth')\n    #ax2.set_axis_off()\n    \n    #ax3.imshow(pred_mask2)\n    #ax3.set_title('UNet-Resnet152 | mIoU {:.3f} | Dice Coef {:.3f} | Pixel Acc {:.3f}'.format(score2, score3, score4))\n    #ax3.set_axis_off()\n    \n    scores.append([score2, score3])\n    \n    table.add_data(wandb.Image(image2), wandb.Image(mask2.astype(np.uint8)), wandb.Image(pred_mask2.astype(np.uint8)), score2, score3)\n    \nrun.log({\"Seagrass Predictions\": table})","metadata":{"execution":{"iopub.status.busy":"2022-09-05T09:01:21.251312Z","iopub.execute_input":"2022-09-05T09:01:21.251797Z","iopub.status.idle":"2022-09-05T09:01:56.710846Z","shell.execute_reply.started":"2022-09-05T09:01:21.251732Z","shell.execute_reply":"2022-09-05T09:01:56.709811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{"execution":{"iopub.status.busy":"2022-09-05T09:02:20.160349Z","iopub.execute_input":"2022-09-05T09:02:20.161419Z","iopub.status.idle":"2022-09-05T09:02:26.826644Z","shell.execute_reply.started":"2022-09-05T09:02:20.161368Z","shell.execute_reply":"2022-09-05T09:02:26.825758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}